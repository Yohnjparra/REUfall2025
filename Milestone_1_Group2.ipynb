{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OeIHZdTXg03B"
      },
      "source": [
        "# Task 1: Upload and clean the data\n",
        "The goal of this task is threefold:\n",
        "\n",
        "\n",
        "1. we want to reduce the noise in the original raw text by removing everything that does not bring information to the language model. everything that is not exactly text: html tags, math equations, urls, etc\n",
        "2. we want to prepare the corpus and make it ready for our language model by tokenizing the text.\n",
        "3. And finally, we want to remove rows with short or very long texts. As you will see, some of the entries are mostly made of large numerical tables. Entries that are too long will not be good reflection of the corpus. Entris that are too short will not bring relevant information to the language model either.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WTh0Auuw-lua",
        "outputId": "56a08e5c-5957-4b7c-f238-2dd59135c845"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting unidecode\n",
            "  Downloading Unidecode-1.4.0-py3-none-any.whl.metadata (13 kB)\n",
            "Downloading Unidecode-1.4.0-py3-none-any.whl (235 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.8/235.8 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip install unidecode # The Unidecode library itself is used to convert Unicode text to plain ASCII characters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PbiEgK7HS4L4"
      },
      "outputs": [],
      "source": [
        "# We only need the following librairies\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "import csv\n",
        "from unidecode import unidecode"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfW0QBdig_k7"
      },
      "source": [
        "Let's load the dataset and shuffle it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YWZx8tbrVrx8"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('https://lp-prod-resources.s3.amazonaws.com/116/other/stackexchange_812k.csv.gz', compression='gzip').sample(frac = 1, random_state = 0).reset_index(drop = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JgzBFBwWvG3q"
      },
      "outputs": [],
      "source": [
        "assert data.shape == (812132, 5), \"The dataset does not have the right dimensions\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WyWqejm3hUE-"
      },
      "source": [
        "And start by exploring the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "eiMeKSHbV7Lv",
        "outputId": "8e3b3449-78bb-4934-89d8-ca646de4fdb7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   post_id  parent_id  comment_id  \\\n",
              "0   291254        NaN    601672.0   \n",
              "1   115372        NaN    221284.0   \n",
              "2   327356        NaN         NaN   \n",
              "3   186923        NaN    355055.0   \n",
              "4   433143        NaN         NaN   \n",
              "\n",
              "                                                text category  \n",
              "0  The condition makes the gradient unbiased. (it...  comment  \n",
              "1                       Yes, that sounds fine to me.  comment  \n",
              "2  <p>Consider gaussian variables belonging to a ...     post  \n",
              "3  Thanks S. Catterall. ^-^ Integrability: I knew...  comment  \n",
              "4               Feature with very few extreme values    title  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-965bfc6e-003f-4b0b-b0f9-847bcfc067a7\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>post_id</th>\n",
              "      <th>parent_id</th>\n",
              "      <th>comment_id</th>\n",
              "      <th>text</th>\n",
              "      <th>category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>291254</td>\n",
              "      <td>NaN</td>\n",
              "      <td>601672.0</td>\n",
              "      <td>The condition makes the gradient unbiased. (it...</td>\n",
              "      <td>comment</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>115372</td>\n",
              "      <td>NaN</td>\n",
              "      <td>221284.0</td>\n",
              "      <td>Yes, that sounds fine to me.</td>\n",
              "      <td>comment</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>327356</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>&lt;p&gt;Consider gaussian variables belonging to a ...</td>\n",
              "      <td>post</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>186923</td>\n",
              "      <td>NaN</td>\n",
              "      <td>355055.0</td>\n",
              "      <td>Thanks S. Catterall. ^-^ Integrability: I knew...</td>\n",
              "      <td>comment</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>433143</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Feature with very few extreme values</td>\n",
              "      <td>title</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-965bfc6e-003f-4b0b-b0f9-847bcfc067a7')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-965bfc6e-003f-4b0b-b0f9-847bcfc067a7 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-965bfc6e-003f-4b0b-b0f9-847bcfc067a7');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-12ea7685-f3da-43b6-87ef-f41d2e66689b\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-12ea7685-f3da-43b6-87ef-f41d2e66689b')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-12ea7685-f3da-43b6-87ef-f41d2e66689b button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bon5CFezh_v1"
      },
      "source": [
        "We have 3 types of text:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "oQK2WV0JWADk",
        "outputId": "ceaff528-d443-41e2-ef40-d94c91e5c246"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "category\n",
              "comment    553076\n",
              "post       167304\n",
              "title       91752\n",
              "Name: count, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>category</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>comment</th>\n",
              "      <td>553076</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>post</th>\n",
              "      <td>167304</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>title</th>\n",
              "      <td>91752</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "data.category.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vR-r4JgDiD5M",
        "outputId": "a37b543f-c0a4-494c-e12f-90dded023d7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------\n",
            "Diebold-Mariano test for predictive accuracy\n",
            "--------------------\n",
            "correlated measurement error in variables\n",
            "--------------------\n",
            "Implementing Predictive Posterior Distribution Using Stan\n"
          ]
        }
      ],
      "source": [
        "# example of titles\n",
        "for p in data[data.category == 'title'].text.sample(3).values:\n",
        "  print('-' * 20)\n",
        "  print(p)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iA47aMGKiSAk"
      },
      "source": [
        "We see that posts text have html tags and latex formatted equations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QagyZuf0iJqO",
        "outputId": "eba760f9-ff2e-4620-bfdc-7b4d6e34aeec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------\n",
            "<p>How do <a href=\"https://stats.stackexchange.com/questions/14474/compendium-of-cross-validation-techniques\">different cross-validation</a> methods compare in terms of model variance and bias? </p>\n",
            "\n",
            "<p>My question is partly motivated by this thread: <a href=\"https://stats.stackexchange.com/questions/61546\">Optimal number of folds in $K$-fold cross-validation: is leave-one-out CV always the best choice?</a>. The answer there suggests that models learned with leave-one-out cross-validation have <strong>higher variance</strong> than those learned with regular $K$-fold cross-validation, making leave-one-out CV a worse choice.</p>\n",
            "\n",
            "<p>However, my intuition tells me that in leave-one-out CV one should see relatively lower variance between models than in the $K$-fold CV, since we are only shifting one data point across folds and therefore the training sets between folds overlap substantially.</p>\n",
            "\n",
            "<p>Or going in the other direction, if $K$ is low in the $K$-fold CV, the training sets would be quite different across folds, and the resulting models are more likely to be different (hence higher variance).</p>\n",
            "\n",
            "<p><strong>If the above argument is right, why would models learned with leave-one-out CV have higher variance?</strong></p>\n",
            "\n",
            "--------------------\n",
            "<p>This usually indicates a problem with Quasi-Complete (categorical variables) or Complete Separation (continuous variables). </p>\n",
            "\n",
            "<p>Quasi-complete:\n",
            "Where you have one category of a class variable x that has only one type of outcome (y=0 or y=11)</p>\n",
            "\n",
            "<p>Complete Separation:\n",
            "some breaking point in a continuous variable where target has only one type of outcome on either side.\n",
            "For example, if $x&gt;5$ then y=1 in all cases and if $x\\leq 5$ then y=0 in all cases. </p>\n",
            "\n",
            "--------------------\n",
            "<p>If s1 is the variance of a small sample, and s2 is the variance of a larger sample from the same population. Is s1 an unbiased estimator for s2?</p>\n",
            "\n",
            "<p>I am thinking since a sample variance is an unbiased estimator of the population variance, then s1 and s2 are both unbiased estimators for sigma^2, so s1 and s2 should also be unbiased estimators for each other.</p>\n",
            "\n",
            "<p>But all the observations in a sample are part of the population, yet the observations between samples doesn't necessarily include each other. Does this make their variances biased?</p>\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for p in data[data.category == 'post'].text.sample(3).values:\n",
        "  print('-' * 20)\n",
        "  print(p)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "No7lYssNiNCy",
        "outputId": "6f07add4-5f07-4dee-dce3-3b7d41e539c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------\n",
            "Not sure, but https://www.sciencedirect.com/science/article/pii/S0047259X15000512 and the MATLAB code at https://www.mathworks.com/matlabcentral/fileexchange/56592-supervised-dimension-reduction?focused=6196200&tab=function may be of interest.\n",
            "--------------------\n",
            "an absolutely phenomenal response and prompt too.  thank you one and all for looking into this for me.\n",
            "--------------------\n",
            "@IrishStat - yes, assume, assume, assume.  But you could relax some assumptions too, just change the code in the lm.shift function some.  I should have spelled out all the assumptions, true  (+1).\n"
          ]
        }
      ],
      "source": [
        "# And here's a sample of comments\n",
        "for p in data[data.category == 'comment'].text.sample(3).values:\n",
        "  print('-' * 20)\n",
        "  print(p)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mk-vHb7LiZrf"
      },
      "source": [
        "# Clean up raw text\n",
        "\n",
        "1. Convert NonAscii characters to their Ascii counterparts\n",
        "2. We're going to remove the following elements:\n",
        "  * html tags\n",
        "  * line returns\n",
        "  * urls\n",
        "  * latex equations\n",
        "  * numbers\n",
        "  * mentions: @someone\n",
        "  * digits\n",
        "  * most of the punctuation\n",
        "  * and extra spaces\n",
        "\n",
        "\n",
        "\n",
        "For that we will use a series of simple regex patterns and the following pandas dataframe pattern:\n",
        "\n",
        "```\n",
        "pattern = r\" some regex pattern\"\n",
        "df.text.apply(lambda t : re.sub(pattern,' ', t) )\n",
        "```\n",
        "\n",
        "Note that it's up to you to decide which elements should be removed or kept. This sequence of transformations can be modified.\n",
        "\n",
        "Not also that the regex patterns we use here are chosen for their simplicity. Feel free to use more precise patterns.  \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qwIOJ_R5EZkw"
      },
      "outputs": [],
      "source": [
        "#Convert NonAscii characters to their Ascii counterparts\n",
        "data['text'] = data.text.apply(lambda t : unidecode(t) )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VAr8qzoKiQjx"
      },
      "outputs": [],
      "source": [
        "# remove html tags\n",
        "data['text'] = data.text.apply(lambda t : re.sub(\"<[^>]*>\",' ', t) )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DS_VHPC1jN5i"
      },
      "outputs": [],
      "source": [
        "# remove line returns\n",
        "data['text'] = data.text.apply(lambda t : re.sub(\"[\\r\\n]+\",' ', t) )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TyQeTFN9jPvK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0ac9589-2105-4a53-b43a-3becba67b00b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:2: SyntaxWarning: invalid escape sequence '\\S'\n",
            "<>:3: SyntaxWarning: invalid escape sequence '\\S'\n",
            "<>:2: SyntaxWarning: invalid escape sequence '\\S'\n",
            "<>:3: SyntaxWarning: invalid escape sequence '\\S'\n",
            "/tmp/ipython-input-3541449220.py:2: SyntaxWarning: invalid escape sequence '\\S'\n",
            "  data['text'] = data.text.apply(lambda t : re.sub(\"http\\S+\",' ', t) )\n",
            "/tmp/ipython-input-3541449220.py:3: SyntaxWarning: invalid escape sequence '\\S'\n",
            "  data['text'] = data.text.apply(lambda t : re.sub(\"http\\S+\",' ', t) )\n"
          ]
        }
      ],
      "source": [
        "# remove urls\n",
        "data['text'] = data.text.apply(lambda t : re.sub(\"http\\S+\",' ', t) )\n",
        "data['text'] = data.text.apply(lambda t : re.sub(\"http\\S+\",' ', t) )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TI1vZWUFjRCT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2b6dde5-153a-4b71-ea10-ecfc7a07ba73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:2: SyntaxWarning: invalid escape sequence '\\S'\n",
            "<>:2: SyntaxWarning: invalid escape sequence '\\S'\n",
            "/tmp/ipython-input-2403395789.py:2: SyntaxWarning: invalid escape sequence '\\S'\n",
            "  data['text'] = data.text.apply(lambda t : re.sub(\"@\\S+\",' ', t) )\n"
          ]
        }
      ],
      "source": [
        "# remove mentions\n",
        "data['text'] = data.text.apply(lambda t : re.sub(\"@\\S+\",' ', t) )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "29nqngCpjST2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75239834-f7a8-432d-be92-6f30b0561f75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:2: SyntaxWarning: invalid escape sequence '\\$'\n",
            "<>:2: SyntaxWarning: invalid escape sequence '\\$'\n",
            "/tmp/ipython-input-1076458036.py:2: SyntaxWarning: invalid escape sequence '\\$'\n",
            "  data['text'] = data.text.apply(lambda t : re.sub(\"\\$[^>]*\\$\",' ', t) )\n"
          ]
        }
      ],
      "source": [
        "# remove latex\n",
        "data['text'] = data.text.apply(lambda t : re.sub(\"\\$[^>]*\\$\",' ', t) )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X2ZBG7PkjTlQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17673d34-1ef8-4f2d-bc95-df34d81b8b94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:2: SyntaxWarning: invalid escape sequence '\\d'\n",
            "<>:2: SyntaxWarning: invalid escape sequence '\\d'\n",
            "/tmp/ipython-input-4030371069.py:2: SyntaxWarning: invalid escape sequence '\\d'\n",
            "  data['text'] = data.text.apply(lambda t : re.sub(\"\\d+\",' ', t) )\n"
          ]
        }
      ],
      "source": [
        "# remove digits\n",
        "data['text'] = data.text.apply(lambda t : re.sub(\"\\d+\",' ', t) )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PEmEZVOwjVmy"
      },
      "outputs": [],
      "source": [
        "# remove some of the punctuation but keep ,.!? and -\n",
        "remove = '\"#$%&()*+/:;<=>@[\\\\]^_`{|}~”“'\n",
        "pattern = r\"[{}]\".format(remove)\n",
        "data['text'] = data.text.apply(lambda t : re.sub(pattern,' ', t) )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DhtXc_dPjX88",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1bf6774-86c3-4439-9e03-00ded4c6e73d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:2: SyntaxWarning: invalid escape sequence '\\s'\n",
            "<>:2: SyntaxWarning: invalid escape sequence '\\s'\n",
            "/tmp/ipython-input-3272314715.py:2: SyntaxWarning: invalid escape sequence '\\s'\n",
            "  data['text'] = data.text.apply(lambda t : re.sub(\"\\s\\s+\",' ', t) )\n"
          ]
        }
      ],
      "source": [
        "# remove multiple spaces\n",
        "data['text'] = data.text.apply(lambda t : re.sub(\"\\s\\s+\",' ', t) )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sYJyjInajZQG"
      },
      "outputs": [],
      "source": [
        "# remove trailing spaces with strip()\n",
        "data['text'] = data.text.apply(lambda t : t.strip() )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nY6RQnqSjbk3"
      },
      "source": [
        "Let's check out the resulting text for the different types:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j7D5HfhXjZ8t",
        "outputId": "3f181795-26be-42f3-e29e-4c8285fb00e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------\n",
            "Help understanding an explanation about minimum description length principle\n",
            "--------------------\n",
            "Do image recognition efforts always rely on machine learning and statistics?\n",
            "--------------------\n",
            "Is there a Correlation metric for Categorical vs Numerical features?\n"
          ]
        }
      ],
      "source": [
        "# titles should not be changed\n",
        "for p in data[data.category == 'title'].text.sample(3).values:\n",
        "  print('-' * 20)\n",
        "  print(p)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_pyo6Bdhjl4J",
        "outputId": "50e3421b-709f-45c2-ad99-81caa0c37073"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------\n",
            "BEGINNING I want to ask a few soft questions about statistics. I understand that maybe probably some all off these questions cannot be answered categorically but I would like to try and get some informed opinions on them anyway. I want to learn some practical statistics and that I could apply to actual problems data sets e.g. Forecasting the outcome of a sporting event or election or back-testing designing some kind of strategy . MIDDLE I'm almost finished an undergraduate degree in Mathematics and Economics and I just don't feel equipped to actually do anything particularly impressive useful reliable with what I've learnt. I have taken all the usual statistics modules in, for example, Classical Inference, Bayesian Inference, Econometrics basically a course on regression , Generalized Linear Models and could understand most everything I did to a level that was more than sufficient for exams, but I still don't feel I'd know or be able implement what's most important. I'd imagine some of these important questions might be What methods models apply where? Why do these methods models apply here? What are the key assumptions you're making? Why might they be wrong? What are the consequences if they are wrong? And then I also have questions like Well what about machine learning neuro-networks and all these other new exciting areas? Should I instead be exploring those areas rather than learning something else? How much statistics is there? How much is useful? How do we know? What's up and coming at the forefront of statistics? what doesn't look like it will stand the test of time? Maybe my question might be better phrased as What's the most useful technique s theories thing s you know? Why? When have you used them what makes them useful? Ok fair enough. I'm convinced. How can I teach myself? Where do I start? What do you foresee being the major up and coming themes in statistics? If it's too difficult to say, how does one begin to gather an educated opinion on the se topic s ? What about the techniques that we already have? How good are they? What are the great accomplishments of statistics to date? How good are we at predicting things and explaining things through numbers? END I have found the amount of statistics available to learn overwhelming. I don't know where to start or why to start anywhere. I'd like to be pointed in the right direction. Books, articles, websites tutorials and resources that you think might be pointing me in the right direction would be greatly appreciated, as would any advice more generally on how to frame this problem better or phrase these questions better would Apologies if this question is poorly phrased and or long and overwritten. I'm very interested in getting to the bottom of some issues raised above. And finally apologies if this question something similar has been asked elsewhere on this site or belongs elsewhere. I had a look around and couldn't find what I was looking for, which is hardly surprising given I'm not quite sure what I'm asking. Many thanks in advance.\n",
            "--------------------\n",
            "I have a categorical variable, var , that can take on values of W, B, A, M, N or P. There are some NAs that I want to impute using the mice package in R, but I know that the missing values cannot be W or B because those people said that they do not belong in that category. I want to impute var but force mice to only choose from everything except B or W . Here is sample code for you to use df data.frame age c , , , , , , , , , , , var c B , W , NA, A ,NA, P , N , NA, M ,NA, B , var categ c , , , , , , , , , , , ht c , , , , , , , , , , imp mice df, remove collinear FALSE Let me know if you need more information.\n",
            "--------------------\n",
            "As a colleague of the authors, I can address this question. To directly answer the OP, is correct the top quotation means that the authors created separate bags each with a random forest of trees -- there will be total trees. As mentioned, there's no clear cut reason why this performed better than bags or random forest. The Weka implementation made it easy to perform all five experiments experiments single tree, bags, bags, random forest, bags of random forests and the authors felt the result was interesting enough to mention. Whether this is something that shows up as a general trend against other datasets such as Kaggle or UCI could be the basis for a good research paper. did not mention it, but there may also be some interplay between the various oob estimates and the final performance metric -- AUC not accuracy . Perhaps this is a trend only observed when optimizing for rank-order and not necessarily discrete predictions.\n"
          ]
        }
      ],
      "source": [
        "# posts should have much less clutter\n",
        "for p in data[data.category == 'post'].text.sample(3).values:\n",
        "  print('-' * 20)\n",
        "  print(p)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KKeBjpmsjxjZ",
        "outputId": "f9f6d2d8-824c-4500-e199-ca362e8e2199"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------\n",
            "The loss is not evaluated on the link scale. This discussion is not the right place for me to offer you a tutorial on GLMs -- I refer you instead to my book on GLMs which Springer will publish in about a month. Nor is this discussion the right place for you to offer an alternative answer to the original question. Write a proper answer if you want to do that.\n",
            "--------------------\n",
            "Thanks for answering. My data is extremely skewed and it is a concern, as you say, for t-tests. What to do in this case?\n",
            "--------------------\n",
            "I think the geometric intuition is clever, but how would one solve this analytically? Specifically, solving for the argmax with our given constraint seems like it could yield any number of forms, and not necessarily the one we are trying to prove.\n"
          ]
        }
      ],
      "source": [
        "# comments should also be less noisy\n",
        "for p in data[data.category == 'comment'].text.sample(3).values:\n",
        "  print('-' * 20)\n",
        "  print(p)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4T1CQqWk6rP"
      },
      "source": [
        "# Tokenize\n",
        "\n",
        "Let's tokenize the text.\n",
        "This will allow us to count the number of tokens of each text and subsequently remove test that are too long or too short.\n",
        "You can use other librairies to tokenize the text (spacy for instance) or other tokenizer. Here we use the [WordPunctTokenizer](https://www.nltk.org/api/nltk.tokenize.html#nltk.tokenize.regexp.WordPunctTokenizer) from NLTK.\n",
        "\n",
        "And we create a new columns called tokens\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ge5LTc3j4Te"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import WordPunctTokenizer\n",
        "tokenizer = WordPunctTokenizer()\n",
        "data['tokens'] = data.text.apply(lambda t : tokenizer.tokenize(t.lower()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Ll09OAjmGby"
      },
      "source": [
        "Let's now count the tokens in each piece of text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r3dp7L-Hl-AR"
      },
      "outputs": [],
      "source": [
        "data['n_tokens'] = data.tokens.apply(len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "id": "dzIE6rdWmPCo",
        "outputId": "a8df1f94-b9c1-4cb9-faa4-67b2c1e00950"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    812132.000000\n",
              "mean         60.065993\n",
              "std          99.396370\n",
              "min           0.000000\n",
              "25%          16.000000\n",
              "50%          35.000000\n",
              "75%          70.000000\n",
              "max       10874.000000\n",
              "Name: n_tokens, dtype: float64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>n_tokens</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>812132.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>60.065993</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>99.396370</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>16.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>35.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>70.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>10874.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> float64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "data.n_tokens.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "PrwoRVQnmRa3",
        "outputId": "f8dc59de-201a-4f19-9820-beef928e12af"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Axes: >"
            ]
          },
          "metadata": {},
          "execution_count": 26
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "data.n_tokens.hist(bins = 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJGNrR5wmV5r"
      },
      "source": [
        "We see that we have some extremely long texts. Let's look at the largest one"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SgsxoSdWmUe3",
        "outputId": "5ac9a8f4-c35e-49ca-b540-125abe02eeda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "My sample includes subjects, of which belong to group L , while the other to group L please see data below . I used GLM for a binary outcome to test for group differences in background variables - summary pre lt - glm L g a m p e, family binomial logit , data df ...yielding significant differences for of them g, a, m, p, and e . So I modeled these background variables as covariates when testing for an association between my predictor, chr and my outcome rsk , in each one of the groups L , L , again using GLM for binary outcome summary fit lt - glm rsk chr g a m p e, family binomial logit , data df which df L , The results showed that a significant association does exist for L but not for L . I would appreciate your help in how to test whether significance non-significance can be attributed to the group condition? . Or in other words, is it true that for subjects L , a significant correlation is evident, while for L ' it's absent. Thanks for responders! Uri structure list L c , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , g c L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L , a c L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L , m c , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , p c , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , e c L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L , chr c L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L , rsk c , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , row.names c L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L , class data.frame\n"
          ]
        }
      ],
      "source": [
        "# this one has a very long series of \"L,\"\n",
        "print(data[data.n_tokens > 10000].text.values[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nK4wiFJamvac"
      },
      "source": [
        "We can see that most of the longest texts are composed of tables with limited semantic value.\n",
        "We will remove rows that have more than an arbitrary number of tokens (let's say 5000) as well as rows that have too few tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cw8QXJ9CmjTo",
        "outputId": "573892a5-c0e4-4b64-9e19-b2a6d6ca33f8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(789649, 7)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "data = data[(data.n_tokens > 4) & (data.n_tokens < 5000)].reset_index(drop = True)\n",
        "data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "P2IOaPeKsDHO",
        "outputId": "54eef16d-286b-41a1-fe52-e70863a2d6a5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "category\n",
              "comment    540582\n",
              "post       165382\n",
              "title       83685\n",
              "Name: count, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>category</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>comment</th>\n",
              "      <td>540582</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>post</th>\n",
              "      <td>165382</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>title</th>\n",
              "      <td>83685</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "data.category.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fe4WDsdFnQ29"
      },
      "source": [
        "# Export data\n",
        "We could export the dataframe as such using a pickle file format.\n",
        "\n",
        "However if we want to keep the original csv format it's going to be easier if we transform the list of tokens into a space separated string.\n",
        "\n",
        "On retrieval we will only have to split the string to get back the list of tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vgqfut5snKj4",
        "outputId": "d89e4e40-c3f4-44d9-84b6-d240abb6bc31"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0    the condition makes the gradient unbiased . it...\n",
              "1                       yes , that sounds fine to me .\n",
              "2    consider gaussian variables belonging to a gau...\n",
              "3    thanks s . catterall . - integrability i knew ...\n",
              "4                 feature with very few extreme values\n",
              "Name: tokens, dtype: object"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data['tokens'] = data.tokens.apply(lambda tk : ' '.join(tk))\n",
        "data.tokens.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wfr2MyKoAtn"
      },
      "source": [
        "And finally let's export the dataframe into a csv file.\n",
        "We will use that csv file as the new cleaned up and filtered out dataset to build our language model in task 2.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vtMO9S-Jn9Nf"
      },
      "outputs": [],
      "source": [
        "data.to_csv('stackexchange_812k.tokenized.csv', quoting = csv.QUOTE_ALL, index = False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6VZYBr8oYEY"
      },
      "source": [
        "# Conclusion\n",
        "Removing or adding steps to this first text processing task will allow us to test different approaches in our language model building process.\n",
        "\n",
        "For instance we can decide not to remove the latex formatted mathematical equation and see if the language model is able to create grammatically valid equations.\n",
        "\n",
        "We could also implement a step to handle contractions (i'm, let's, ...) and see if that improves the quality of the generated text\n",
        "\n",
        "Finally we could also decide to work on the vocabulary and filter out typos or non-English unknown words using named entity recognition to tag specific tokens.\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}